# Deep Learning Configuration
# Inherits from base_config.yaml

defaults:
  - /base_config

# Override base settings for deep learning
experiment:
  type: "deep_learning"
  name: "baseline_cnn"
  description: "Baseline CNN for end-to-end audio classification"

# Model architecture
model:
  name: "baseline_cnn"

  # Input settings
  input:
    type: "mel_spectrogram"  # raw_audio, mel_spectrogram, mfcc
    n_mels: 128
    n_fft: 2048
    hop_length: 512
    max_length: 1876  # ~30 seconds at 16kHz with hop_length=512

  # CNN architecture
  cnn:
    # Convolutional layers
    conv_layers:
      - filters: 32
        kernel_size: [3, 3]
        stride: [1, 1]
        padding: "same"
        activation: "relu"
        batch_norm: true
        dropout: 0.1

      - filters: 64
        kernel_size: [3, 3]
        stride: [2, 2]
        padding: "same"
        activation: "relu"
        batch_norm: true
        dropout: 0.1

      - filters: 128
        kernel_size: [3, 3]
        stride: [2, 2]
        padding: "same"
        activation: "relu"
        batch_norm: true
        dropout: 0.2

      - filters: 256
        kernel_size: [3, 3]
        stride: [2, 2]
        padding: "same"
        activation: "relu"
        batch_norm: true
        dropout: 0.2

    # Global pooling
    global_pooling: "avg"  # avg, max, both

    # Fully connected layers
    fc_layers:
      - units: 512
        activation: "relu"
        dropout: 0.5
        batch_norm: true

      - units: 256
        activation: "relu"
        dropout: 0.3
        batch_norm: true

    # Output layer (will be set to num_classes automatically)
    output_activation: null  # will use softmax for classification

# Data augmentation
data_augmentation:
  enabled: true

  # Time domain augmentations
  time_domain:
    # Time shifting
    time_shift:
      enabled: true
      max_shift: 0.1  # fraction of total length

    # Speed perturbation
    speed_perturbation:
      enabled: false
      factors: [0.9, 1.1]

  # Frequency domain augmentations
  frequency_domain:
    # Frequency masking
    freq_mask:
      enabled: true
      max_mask_pct: 0.1
      num_masks: 2

    # Time masking
    time_mask:
      enabled: true
      max_mask_pct: 0.1
      num_masks: 2

    # Mixup
    mixup:
      enabled: false
      alpha: 0.2

# Optimizer settings
optimizer:
  name: "adam"
  learning_rate: 0.001
  weight_decay: 0.0001

  # Adam specific
  betas: [0.9, 0.999]
  eps: 1e-8

  # Alternative optimizers
  alternatives:
    sgd:
      momentum: 0.9
      nesterov: true

    adamw:
      betas: [0.9, 0.999]
      eps: 1e-8

# Learning rate scheduler
lr_scheduler:
  name: "reduce_on_plateau"

  # ReduceLROnPlateau settings
  reduce_on_plateau:
    mode: "max"
    factor: 0.5
    patience: 5
    min_lr: 1e-6
    verbose: true

  # Alternative schedulers
  alternatives:
    cosine_annealing:
      T_max: 50
      eta_min: 1e-6

    step_lr:
      step_size: 30
      gamma: 0.1

# Loss function
loss:
  name: "cross_entropy"
  label_smoothing: 0.0

  # Alternative losses
  alternatives:
    focal_loss:
      alpha: 1.0
      gamma: 2.0

# Training settings (override base)
training:
  batch_size: 32
  num_epochs: 100
  early_stopping_patience: 15
  gradient_clip_norm: 1.0

  # Validation frequency
  validate_every: 1  # epochs

  # Checkpoint saving
  save_every: 10  # epochs
  save_best_only: true

# Model compilation
compile:
  enabled: false  # torch.compile for PyTorch 2.0+
  mode: "default"  # default, reduce-overhead, max-autotune

# Mixed precision training
mixed_precision:
  enabled: false
  dtype: "float16"