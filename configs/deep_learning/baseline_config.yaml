# Deep Learning Configuration
# Inherits from base_config.yaml

defaults:
  - /base_config
  - _self_

# Override base settings for deep learning
experiment:
  type: "deep_learning"
  name: "CNN"
  description: "Various CNN structures for end-to-end audio classification"

# Model architecture
model:
  name: "ShortChunkCNN"  # ShortChunkCNN or ShortChunkCNN_Res
  n_channels: 128
  n_fft: 1024  # Reduced from 2048 to handle shorter audio chunks
  hop_length: 256  # Reduced proportionally
  f_min: 80.0
  f_max: 8000.0
  n_mels: 80
  n_class: 20

  # name: "ShortChunkCNN_Res"
  # n_channels: 128
  # n_fft: 1024
  # hop_length: 256
  # f_min: 80.0
  # f_max: 8000.0
  # n_mels: 80
  # n_class: 20


# Data augmentation
data_augmentation:
  enabled: true

  # Time domain augmentations
  time_domain:
    # Time shifting
    time_shift:
      enabled: true
      max_shift: 0.1  # fraction of total length

    # Speed perturbation
    speed_perturbation:
      enabled: false
      factors: [0.9, 1.1]

  # Frequency domain augmentations
  frequency_domain:
    # Frequency masking
    freq_mask:
      enabled: true
      max_mask_pct: 0.1
      num_masks: 2

    # Time masking
    time_mask:
      enabled: true
      max_mask_pct: 0.1
      num_masks: 2

    # Mixup
    mixup:
      enabled: false
      alpha: 0.2

# Optimizer settings
optimizer:
  name: "adam"
  learning_rate: 0.001
  weight_decay: 0.0001

  # Adam specific
  betas: [0.9, 0.999]
  eps: 1e-8

  # Alternative optimizers
  alternatives:
    sgd:
      momentum: 0.9
      nesterov: true

    adamw:
      betas: [0.9, 0.999]
      eps: 1e-8

# Learning rate scheduler
lr_scheduler:
  name: "reduce_on_plateau"

  # ReduceLROnPlateau settings
  reduce_on_plateau:
    mode: "max"
    factor: 0.5
    patience: 5
    min_lr: 1e-6

  # Alternative schedulers
  alternatives:
    cosine_annealing:
      T_max: 50
      eta_min: 1e-6

    step_lr:
      step_size: 30
      gamma: 0.1

# Loss function
loss:
  name: "cross_entropy"
  label_smoothing: 0.0

# Training settings (override base)
training:
  batch_size: 16
  num_epochs: 100
  early_stopping_patience: 15
  gradient_clip_norm: 1.0

  # Validation frequency
  validate_every: 1  # epochs

  # Checkpoint saving
  save_every: 15  # epochs
  save_best_only: true

# Model compilation
compile:
  enabled: false  # torch.compile for PyTorch 2.0+
  mode: "default"  # default, reduce-overhead, max-autotune

# Mixed precision training
mixed_precision:
  enabled: false
  dtype: "float16"